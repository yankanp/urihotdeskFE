# MLSS
# 2022-02-09
Lead Scribe: CKA
## Paper - Graphical Models for Inference with Missing Data
#  [paper](https://proceedings.neurips.cc/paper/2013/file/0ff8033cf9437c213ee13937b1``c4c455-Paper.pdf) -- Presented by Chamudi
 
### Graphical Models for Inference with Missing Data

#### Missing data can have several hamrful consequences. 
- Firstly they can significantly bias the outcome of research studies. 
    - This is mainly because the response profiles of non-respondents and respondents can be significantly different from each other. Hence ignoring the former distorts the true proportion in the population. 
- Secondly, performing the analysis using only complete cases and ignoring the cases with missing values can reduce the sample size there by substaintially reducing estimation efficiency. 
- Lastly, many of the algorithms and statsitical techniques are generally tailored to draw inferences from complete datasets. 
    - it may be difficult or even inappropriate to apply these algorithms and statistical techniques on incomplete datasets. 
        
#### Existing Methods for Handling Missing Data 
- Listwise deletion (LD) and pairwise deletion (PD) are used in approximately 96% of studies in the social and behavioral sciences 
- Expectation-maximization (EM) [example: K-Means] algorithm is a general technique for finding max likelihood (ML) estimates from incomplete data. 
- ML is often used in conjunction with imputation methods 
    - Mean Sub, Hot-deck imputation, cold-deck imputation, and Multiple Imputation (MI). 
- This paper aims to iluminate missing dat aproblems using casual graphs 
- The objectives are: 
    - Given a target relation Q to be estimated and a set of assumptions about the missingess process encoded in a graphical model, (i) under what conditions does a consistent estimate exist, and (ii) how can we product it from the data available. These questions are answered with the aid of `Missingness Graphs (m-graphs)` 
    - Review the traditional taxonomy of missing data problems and cast it in graphical terms. 
    - define the`notion of recoverability` - the existence of a consistent estimate - and present graphical conditions for detecting recoverability of a given probalistic query Q. 

#### Graphical Representation of the Missingness Process
- Graphical Models have been used to analyze missing missing information in the form of missing cases 
- The need exists for a general approach capable of modeling an arbitrary data-generating process and deciding whether how missingness can be outmaneuvered in every dataset generated by that process. 
- Such a general approach should allow each variable to be governed by its own missingness mechanism, and each mechanism to be triggered by other partially observed variables in the model. 
- To achieve this flexibility we use a graphical model called `missingness graph` which is a `DAG (Directed Acyclic Graph)` defined as follows.
#### Missingness Graphs
![](https://i.imgur.com/368coCZ.png)
Let G(ùïç, E) be the causal DAG where ùïç = V ‚à™ U ‚à™ V* ‚à™ R

**V** is the set of observable nodes. Nodes in the graph correspond to variables in the data set. 
**U** is the set of unobserved nodes 
**E** is the set of edges in the DAG 
**V*** is a set of all proxy variables
**‚Ñù** is the set of all causal mechanisms that are responsible for missingness

* Oftentimes we use bi-directed edges as a shorthand notation to denote the existence of a U variable as common parent of two variables in Vo ‚à™ Vm ‚à™ ‚Ñù. 

* **V** is partitioned into **Vo** and **Vm** such that 
* **Vo ‚äÜ V** is the set of variables that are observed in all records in the population 
* **Vm ‚äÜ V** is the set of variables that are missing in at least one record. 
* Variable **X** is termed as **fully observed** if **X‚ààVo** and **partially observed** if **X‚ààVm**.

![](https://i.imgur.com/y9u5fuJ.png)

* This graphical representation briefly shows both the causal relationships among variables in V and the process that accounts for missingness in some of the variables.
* Since every d-separation in the graph implies conditional independence in the distribution, the m-graph provides an effective way of representing the statistical properties of the missingness process and, hence, the potential of recovering the statistics of variables in Vm from partially missing data.

#### Taxonomy of Missingness Mechanisms
It is common to classify missing data mechanisms into three types
* **Missing Completely At Random (MCAR)** : Data are MCAR if the probability that Vm is missing is independent of Vm or any other variable in the study, as would be the case when `respondents decide to reveal their income levels based on coin-flips`. 
* **Missing At Random (MAR)** : Data are MAR if for all data cases Y , P(R|Yobs, Ymis) = P(R|Yobs) where Yobs denotes the observed component of Y and Ymis, the missing component. Example: `Women in the population are more likely to not reveal their age`. 
* **Missing Not At Random (MNAR)** or ‚Äúnon-ignorable missing‚Äù: Data that are neither MAR nor MCAR are termed as MNAR. Example: `Online shoppers rate an item with a high probability either if they love the item or if they dislike it. In other words, the probability that a shopper supplies a rating is dependent on the shopper‚Äôs underlying liking`.
![](https://i.imgur.com/KoH81yk.png)

* In the graph-based interpretation used in this paper, MCAR is defined as total independence between **‚Ñù and Vo ‚à™ Vm ‚à™ U** i.e. **‚Ñù ‚àê (Vo ‚à™ Vm  ‚à™ U)**, as shown in Figure `a`.
* MAR is defined as independence between **‚Ñù and Vm ‚à™ U given Vo**  i.e. **‚Ñù ‚àê Vm ‚à™ U|Vo**, as shown in Figure `b`. 
* Finally if neither of these conditions hold, data are termed MNAR, as shown in Figure `c` and `d`.

#### Recoverability 
Recoverability is a measurement to see if we can get P(X, Y) from the entire dataset D. Examine the conditions under which a bias-free estimate of a given probabilistic relation Q can be computed.

**Definition**  (Recoverability). Given a **m-graph G**, and a **target relation Q** defined on the variables in **V** , **Q** is said to be **recoverable** in **G** if there `exists an algorithm that produces a consistent estimate of Q for every dataset D` such that **P(D)** is `(1) compatible with G` and `(2) strictly positive over complete cases` i.e. **P(Vo, Vm, ‚Ñù = 0) > 0**.

#### Recoverability when data are MCAR
For MCAR data we have ‚Ñù ‚àê (Vo ‚à™ Vm). Therefore, we can write 
P(V ) = P(V | ‚Ñù) = P(Vo, V* |‚Ñù = 0). 

Since both **‚Ñù** and **V*** are observables, the joint probability **P(V)** is **consistently estimable (recoverable)** by considering complete cases only 

*Example : Let X be the treatment and Y be the outcome as depicted in the m-graph in Fig. 1 (a). Let it be the case that we accidentally deleted the values of Y for a handful of samples, hence Y ‚àà Vm. Can we recover P(X, Y )?*

From **D**, we can compute **P(X, Y* , Ry)**. 
From the m-graph G, we know that **Y*** is a collider and hence by d-separation, 
**(X ‚à™ Y ) ‚àê Ry**. Thus **P(X, Y ) = P(X, Y |Ry)**. 
In particular, **P(X, Y ) = P(X, Y |Ry = 0)**. 
When **Ry = 0**, *by eq. (1)*, **Y* = Y** . 
Hence,                **P(X, Y )** **= P(X, Y*** **|Ry = 0)**                  `(2)` 
The RHS of Eq. 2 is consistently estimable from D;
 **hence P(X, Y ) is recoverable**.


#### Recoverability when data are MAR 

When data are **MAR**, we have **R ‚àê Vm|Vo**. 
Therefore   **P(V )   =   P(Vm|Vo)P(Vo)   =   P(Vm|Vo, R = 0)P(Vo)**. 
Hence the joint distribution **P(V)** is recoverable.

*Example : Let X be the treatment and Y be the outcome as shown in the m-graph in Fig. 1 (b). 
Let it be the case that some patients who underwent treatment are not likely to report the outcome, hence the arrow X ‚Üí Ry. Under the circumstances, can we recover P(X, Y )?*

From **D**, we can compute **P(X, Y* , Ry)**. 
From the m-graph **G**, we see that **Y*** is a collider and X is a fork. 
Hence by d-separation, **Y ‚àê Ry|X**. 
Thus        **P(X, Y )   =   P(Y |X)P(X)   =   P(Y |X, Ry)P(X)**.
In particular,   **P(X, Y )   =   P(Y |X, Ry = 0)P(X)**. 
When  **Ry = 0**,       by eq. (1),       **Y*** **= Y**. 
Hence,       **P(X, Y )   =   P(Y*** **|X, Ry = 0)P(X)**               `(3)` 
and since **X is fully observable, P(X, Y ) is recoverable**


#### Recoverability when data are MNAR

Data that are neither MAR nor MCAR are termed **MNAR**. Though it is generally believed that relations in MNAR datasets are not recoverable, the following example demonstrates otherwise.

*Example : Fig. 1 (d) depicts a study where (i) some units who underwent treatment (X = 1) did not report the outcome (Y ) and (ii) we accidentally deleted the values of treatment for a handful of cases. Thus we have missing values for both X and Y which renders the dataset MNAR. We shall show that P(X, Y ) is recoverable.*

![](https://i.imgur.com/plJibIl.png)


From D, we can compute **P(X***, **Y***, **Rx, Ry)**. 
From the m-graph G, we see that **X ‚àê Rx** and **Y ‚àê (Rx ‚à™ Ry)|X**. 

Thus **P(X, Y )  =  P(Y |X)P(X)  =  P(Y |X, Ry = 0, Rx = 0)P(X|Rx = 0)**. 
When **Ry = 0** and **Rx = 0** we have (*by Equation (1)* ), **Y* = Y** and **X* = X**. 

Hence,  **P(X,Y) =**   **P(Y*** **|X*** , **Rx = 0** , **Ry = 0)P(X*** **|Rx = 0)**              `(4)` 
Therefore, **P(X,Y) is recoverable**


#### Conditions for Recoverability 
How can we determine if a given relation is recoverable? 
```Theorem 1: A query Q defined over variables in V0 ```
Theorem 1 provides a sufficient condition for recoverability

**Theorem 1** 
`A query Q defined over variables in Vo ‚à™ Vm is recoverable if it is decomposable into terms of the form Qj = P(Sj |Tj ) such that Tj contains the missingness mechanism Rv = 0 of every partially observed variable V that appears in `

**Proof:** 
`If such a decomposition exists, every Qj is estimable from the data, hence the entire expression for Q is recoverable.`

*Example : Consider the problem of recovering Q = P(X, Y ) from the m-graph of Fig. 3(b).*

![](https://i.imgur.com/uYApjr3.png)

Attempts to decompose Q by the chain rule, as was done in Eqs. (3)
`P(X, Y )   =   P(Y* |X, Ry = 0)P(X)    (3)` 
and (4) would not satisfy the conditions of Theorem 1.

To witness we write P(X, Y ) = P(Y |X)P(X) and note that the graph does not permit us to augment any of the two terms with the necessary Rx or Ry terms; 

X is independent of Rx only if we condition on Y , which is partially observed, and Y is independent of Ry only if we condition on X which is also partially observed. 

This deadlock can be disentangled however using a non-conventional decomposition:

![](https://i.imgur.com/mn7ErCU.png)

where the denominator was obtained using the independencies **Rx ‚àê (X, Ry)|Y** and **Ry ‚àê (Y, Rx)|X** shown in the graph. 

The final expression above satisfies **Theorem 1** and renders **P(X,Y) recoverable**. 

This example again shows that ***recovery is feasible*** even when data are **MNAR**.

*Theorem 2* operationalizes the decomposability requirement of *Theorem 1*.

**Theorem 2** 

`(Recoverability of the Joint P(V)).Given a m-graph G with no edges between the R variables and no latent variables as parents of R variables, a necessary and sufficient condition for recovering the joint distribution P(V) is that no variable X be a parent of its missingness mechanism Rx. Moreover, when recoverable, P(V) is given by`

![](https://i.imgur.com/t7didV4.png)

*Theorem 3* gives a sufficient condition for recovering the joint distribution in a Markovian model.

**Theorem 3**
`Given a m-graph with no latent variables (i.e., Markovian) the joint distribution P(V) is recoverable if no missingness mechanism Rx is a descendant of its corresponding variable X. Moreover, if recoverable, then P(V) is given by`

![](https://i.imgur.com/VKRDXTd.png)

**Theorem 4**
`A sufficient condition for recoverability of a relation Q is that Q be decomposable into an ordered factorization, or a sum of such factorizations, such that every factor Qi = P(Yi |Xi) satisfies Yi ‚àê (Ryi , Rxi )|Xi . A factorization that satisfies this condition will be called admissible.`

*Theorem 4* will allow us to confirm recoverability of certain queries Q in models such as those in Figures `a`,  `c`, `d`  which do not satisfy the requirement in *Theorem 2*

![](https://i.imgur.com/FtykI3w.png)

P(X|Y ) = P(X|Rx = 0, Ry = 0, Y) is recoverable

![](https://i.imgur.com/GlnLtiP.png)

P(X, Y, Z)  =  P(Z|X, Y, Rz = 0, Rx = 0, Ry = 0) P(X|Y, Rx = 0, Ry = 0) P(Y |Ry = 0) is recoverable

![](https://i.imgur.com/QIaMoJ4.png)

P(X, Z) = P(X, Z|Rx = 0, Rz = 0) is recoverable

#### Conclusion
* `Causal graphical models depicting the data generating process can serve as a powerful tool for analyzing missing data problems`
* `Formalized the notion of recoverability and showed that relations are always recoverable when data are missing at random (MCAR or MAR) and even when data are missing not at random (MNAR).` 
* `presented a sufficient condition to ensure recoverability of a given relation Q (Theorem 1) and operationalized Theorem 1 using graphical criteria (Theorems 2, 3 and 4).`

# 2022-02-07

Lead Scribe: Surbhi Rathore

## Opening Notes
Handling missing data

## Paper - Handling Missing Values when Applying Classification Models
 [paper](https://www.jmlr.org/papers/volume8/saar-tsechansky07a/saar-tsechansky07a.pdf) -- Presented by Emmely 
 
### Analysis over different treatments of missing values at prediction time.
 
 - Alternative courses of action when features are missing:
     - Discard instances: Simply discarding instances with missing values.
     - Acquire missing values: In practice, a missing value may be obtainable by incurring a cost, such as the cost of performing a diagnostic test or the cost of acquiring consumer data from a third party.
     - Imputation : The main idea of imputation is that if an important feature is missing for a particular instance, it can be estimated from the data that are present.
         - (Predictive) Value Imputation (PVI): Value imputation estimates a value to be used by the model in place of the missing feature. Value imputation is more common in the statistics community
         - Distribution-based Imputation (DBI): Distribution-based imputation estimates the conditional distribution of the missing value, and predictions will be based on this estimated distribution. distribution-based imputation is the basis for the most popular treatment used by the (non-Bayesian) machine learning community 
         - Unique-value imputation: Rather than estimating an unknown feature value it is possible to replace each missing value with an arbitrary unique value.
     - Reduced-feature Models: Reduced-feature models : We refer to these models as reduced-feature models, as they are induced using only a subset of the features that are available for the training data. Reduced-feature models can be computationally expensive.

- Missing Completely At Random (MCAR) : refers to the scenario where missingness of feature values is independent of the feature values (observed or not). (missing values have nothing to to do with feature values)

#### Comparison of PVI, DBI and Reduced Modeling
- Reduced-feature modeling performs consistently outperforms the other two method. 

#### LOW and HIGH FEATURE IMPUTABILITY Result
- PVI is better for higher feature imputability, and DBI is better for lower feature imputability. Value imputation generally preferable for high feature imputability, and DBI generally better for low feature imputability.

#### REDUCED-FEATURE MODELING SHOULD HAVE ADVANTAGES ALL ALONG THE IMPUTABILITY SPECTRUM
- Reduced modeling is a lower-dimensional learning problem than the modeling to which imputation methods are applied, it will tend to have lower variance and thereby may exhibit lower generalization error.

####  Evaluation with ‚ÄúNaturally Occurring‚Äù Missing Values
-  By ‚Äúnaturally occurring,‚Äù we mean that these are data sets from real classification problems, where the missingness is due to processes of the domain outside the control.

### Conclusions
```{epigraph}
Reduced-feature models are preferable both to distribution-based imputation and 
to predictive value imputation.Reduced models undertake a lower-variance learning 
task, and do not fall prey to certain pathologies. Predictive value imputation 
and DBI are easy to apply, but one almost always pays‚Äîsometimes dearly‚Äîwith 
suboptimal accuracy.
```
## Paper - Missing data imputation using statistical and machine learning methods in a real breast cancer problem
 [paper](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.701.4234&rep=rep1&type=pdf) -- Presented by Chan 

### The ‚Äò‚ÄòEl Alamo-I‚Äô‚Äô breast cancer dataset
- one of the largest databases on breast cancer in Spain.
- The missing data represent 5:61% of the overall data set.

### Statistical methods
- The statistical imputation methods include mean imputation, hot-deck, and MI methods based on regression and the expectation maximisation (EM) algorithm.
    - Mean Imputation: is a simple application of regression imputation, the mean value of each non-missing variable is used to fill in missing values for all observations.
    -  Hot-deck imputation : nearest neighbour hot-deck imputation is applied, where a nonrespondent is assigned the value of the nearest neighbour record according to a similarity criterion.
    -   Multiple imputation : , MI replaces an unknown value with a set of plausible data and uses an appropriate model that incorporates random variation. MI has several desirable features: 
        -   (1) an appropriate random error is introduced into the imputation process to obtain approximately unbiased estimates of all parameters; 
        -   (2) good estimates of standard errors are obtained from repeated imputation; and 
        -   (3) MI can be used for any kind of data and any kind of analysis without specialised software.

### Machine learning methods

- Imputation methods based on machine learning are sophisticated procedures that generally consist of creating a predictive model to estimate values that will substitute the missing items.
- These approaches model the missing data estimation based on information available in the data set. 
- Three well-known imputation techniques using machine learning approaches: MLP, SOM and KNN.
    - Multi-layer perceptron : 
        - An MLP consists of multiple layers of computational units interconnected in a feed-forward way. 
        - Each unit in one layer is directly connected to the neurons of the subsequent layer. 
        - A fully connected, two-layered MLP architecture was used and sigmoidal activation functions.
        - MLP networks can be used to estimate missing values by training an MLP to learn the incomplete features (used as outputs),using the remaining complete features as inputs.
    - Self-organisation maps : 
        - An SOM is a neural network model made out of a set of nodes (or neurons) that are organised on a 2D grid and fully connected to the input layer.
        - The training of a basic SOM is performed using an iterative process. After the weight vectors are initialised, they are updated using all input training vectors.
        - After the SOM model has been trained, it can be used to estimate missing values. 
    - K-nearest neighbours : 
        -  the KNN imputation algorithm uses only similar cases with the incomplete pattern. 
        -  Given an incomplete pattern x, this method selects the K closest cases that are not missing values in the attributes to be imputed (i.e., features with missing values in x), such that they minimise some distance measure.
        -  The optimal value of K is usually chosen by crossvalidation.
        -  Calculation of replacement value depends on the type of data;
            - example, the mode is selected for discrete data (categorical data),
            - while the mean is used for numerical data (continuous data).
    - The ANN prognosis model : 
        - numerical simulations are performed on the data imputed by the methods described above on neural networks comprising a single hidden layer with the number of neurons between 2 and 50.
        
####  Model evaluation
- The accuracy of the prognosis models is evaluated by testing two main properties: 
    - discrimination and calibration. Discrimination is the ability to separate patients with and without a relapse event.
    - Calibration is the ability to correctly estimate the risk or probability of a future event. 

### Conclusions
```{epigraph}
machine learning techniques may be the best approach to imputing missing values, 
as they led to statistically significant improvements in prediction accuracy. 
Imputation techniques depend on the available data and the prediction model used. 
Also, these results might not generalise to different data sets

```

## closing 

- Volunteers for paper presentation: Chamudi & Lily 

--------------------------------------------------------------------------------------


# MLSS

# 2022-02-02
Lead Scribe: Damon Coffey

Roles for computing in social change
-concerns about fairness, bias and accountability in the field

Introduction:
- high stakes decision making algorithms have potential to predict outcomes more accurately
- cs has generally failed to target the correct point of intervention
- ex: intervention at the selection phase in employment context could prevent a hostile work place

Computing as a Diagnostic
- computing can help us measure social problems and diagnose how they manifest in tech systems
- computing cannot solve issues on its own
- Diagnostics work can be valuable
    - highlight tech dimensions of social problems
- misinformation can negatively affect marginalized populations more ex: search engines displaying low quality health information

- not presented as solutions, rather as tools to document practices
    - not to confuse diagnostics with treatment
    - computing is not unique in helping diagnose social problems
        - sociology, etc..
    - certain tools can be treated as certainity for every situation, which is not the case

Computing as a Formalizer
- computing requires explicit specification of inputs and goals
- these inputs and goals can be affected by transperency, accountability and stake holder participation
    - need to be precise
ex: risk assessment: debate over how to formalize pretrial risk, if and how to use these instruments
- not all data is easy to quantify
- may press people to rely on measures that are incorrect

Computing as Rebuttal
- computing can clarify the limits of technical interventions and of policies promised on them
- limits of computing can drive people to reject computational approaches
- ex: using an algotrithm to determine an immigrants societal worth, not good. Should seek a differnt method rather than forcing a technological one
- need to understand what algorithms are actually capable of, instead of forcing it on everything
    - need to show what an algorthm CANT do (prove limits)
- prediction algorithms for risk assessment
- computational research on fairness is built on discrimination law
- Risks
    - proclomations of what a computational tool is incapable of may focus on improving tool even if it is not possible

Computing as a Synecdoche
- computing can foreground long standing social problems in a new way
- Eubank's core concern: computing is just one mechanism through which longstanding poverty policy is manifested
- Automated systems can divert poor people from the resources they need
- computing can help bring attention to old problems, however

- synecdochal focus on computing must walk a pragmatic line between over emphasis on tech aspects and recognition of the work tech actually does
- need to find a balance between the two and develop better systems with more emphasis on social issues







--------------
# 2022-01-31

Lead Scribe: Derek Jacobs

## admin 

- grading contract will be posted in time for Wednesday
- notes & posting [example](https://github.com/ml4scisoc/ml4scisoc.github.io/pull/4)
- can leave out admin in notes; that material wil mostly be other places

## Opening Notes

- set the stage for how we think about other work and setting up your projects



## Scientific Method in the Science of Machine Learning 
 [paper](https://arxiv.org/abs/1904.10922) 


### Introduction
- ML is having a hard time explaining some results
- Replications are failing 


### Scientific Method

```{epigraph}
Starting from the assumption that there exists accessible ground truth, the scientific method is a systematic framework for experimentation that allows researchers to make objective statements about
phenomena and gain knowledge of the fundamental workings of a system under investigation.

-- Jessica Zosa Forde and Michela Pagnini
```
- process and a social contract
    - ML might not be systematic
        - Control the randomness of our algorithms
    - Procedures/Trust allow comparisons of results
- assumes a ground truth
    - Things can go wrong if there simply isn\'t a ground truth
    - Especially when using ML in social context
- Hypotheses
    - A formed "guess"
    - In CS, the learning process is as follows
        - Here's a problem (that has an answer)
        - Write code to solve it to specifications
    - Hypotheses can be falsified through statistical and other analysis
        - You can prove the opposite is not true, but challenging to prove truth

you can express hypotheses as priors, but that prior wouldnt be the same in the rest of ML lit

```{admonition} Discuss
When might this not apply? 
```


```{epigraph}
At the base of scientific research lies the notion that an experimental outcome is a random variable, and that appropriate statistical machinery must be employed to estimate the properties of its
distribution. 
```
 
- Treat your accuracies as a random variable as part of an experiment and you're experimenting around those
- In stats people chase having p values <= 0.05 and so we have a reproducibility crisis
- [Reproducibility Study](https://osf.io/ezcuj/wiki/home/)
    - < 40% replicated the original results

- there are some problems with NHST but there are alternatives that are still rigorous
    - [for design research](https://schmettow.github.io/New_Stats/index.html#what-new-stats)
    - [in psych](https://www.psychologicalscience.org/members/new-statistics)
    - [bayesian in hci](https://www.mjskay.com/papers/chi_2016_bayes.pdf)
        - Bayesian
            - Priors allowed
            - More broadly defined
            - Everything we do is influenced by our thoughts and so there's some subjectivity
            - Interpreting results in terms of previous results
        - Frequentist
            - No Priors
            - Probabilities strictly refer to events like fair coin flips (100 flips we expect 50 heads 50 tails)
            - Knowledge does not accrue

### Case study

HEP:
- proposed thoery
- null hypothesis
- cafeull accounting
- model building and hypothesis testing phases
- parametric models derived from first prcinicples
- statistical test is constructed

ML Analogy: 
- suspect new activation
- formulate quantitative hypothesis
    - How will it work, what'll it do, how much will it improve
    - And behavior of how it'll change
- run experiments
    - Record outcomes from base models (no intervention)
- This is a statistical model of your experiment, not an actual model
    - Dataset, optimizer, init, hyperparameters, etc are just noise in the question of "does the activation function work"
    - Or make things specific (improve results with a specific dataset)

### Recommendations


**1. formulate hypotheses first
2. statistical testing**
3. operate in controlled,
reproducible, and verifiable settings
4. negative result workshops 
    - [pregistration workshop at neurips 2021](https://preregister.science/neurips2020.html) not an author as speaker
        - If your experimental plan is good, results are published regardless of positive or negative case
    - [new journal](https://www.jmlr.org/tmlr/editorial-policies.html)

## Value Laden Shifts in ML

[paper](https://arxiv.org/abs/1912.01172)
@brownsarahm reminder from lily
**talk on incorporating ethics into teaching data structs - environmental cost of algorithms**

- Looking at how different values influence what is being done in ML
- disciplinary shifts are not objective, but value laden
- Model Types
    - Different categories of models typically used for specific purposes
    - The structure of what our learning algorithm outputs
    - e.g. CNN for image processing, Linear Models, SVMs, etc

### Model types as organizing 

- many researchers self-organize in types; eg for revieing, workshops etc
    - How do we organize them in a package like sklearn, but also "who knows who works on this"
- commitment is fueled by exemplars
- has down stream effects: 
    - guide research agenda and problem selection
        -  How model types influence problem selection
            - Different types are tailored to different problems
            - Most popular model means most funding
            - More funding on specific models drives improvements to that area specifically and other problems get lost
    - constrain search for solutions
    - prerequisites, eg deep learning  and data volume (fig 1) and compute power (fig2)
        - fig1
            - Depending on data amount, we may favor one model over the other, and that in turn is researched more
        - fig2
            - Shows over time how my computer power is used in training AI
            - Recently exponential growth
- model types have parallels but important differences in philosophical scientific organzing principles
    - decreases theory development
    - Kuhn
        - The organizing paradigm defines what questions are valid
        - For example
            - If we only have earth water fire ether, we can't ask what molecules do
- When committed to model types...
    - The whole industry shifts towards it
    - Like NVidia GPUs, computer architectures, etc

### Model type is self-reinforcing 

- comparing them is influenced by the model type points above (problems, prerequisites)

### Comparison between types is value laden 

- Applied to ImageNet
    - Benchmark image classification problem
    - Until 2011, best error rate was 25% (no deep learning)
    - 2012 - AlexNet reached 16% (deep learning)
    - By 2016 ImageNet is basicaclly "done" due to all extremely high accuracies (97% vs 97.1%)
- This doesn't mean the problem of image classification is solved
- prerequisites
    - compute
        - Who has access
    - data
        - Who has access
        - What sets are created/curated
- Evaluation criteria
    - in theories: as a whole, internal consistent, predictive, etc

```{epigraph}
Evaluating theories based on their theoretical
virtues is a value-laden activity when theoretical virtues are carriers
of values.

-- Milli and Dotan
```
- eg consistency requires keeping the bad from the old in new
- eg: metrics and discrimination
    - Something that was sexist previously is still now

### Conclusion

```{epigraph}
A related question inspired by these issues is who should make
decisions in what values are furthered? Who gets to have a voice?
In talking about selection of problems in science, Kitcher (2011)
argues that all sides should have a say, including laypersons [71].
A question for machine learning is: is the same true for machine
learning? Who should have a say about which criteria are important in evaluating model-types? That is itself another value-laden
question.
```

## Overall Paper thoughts

```{admonition} Discuss 
General thoughts?
```
- Think about things that're interesting, confusing, etc (for future reference)
- Take care when conducting ML
    - Feed in data, hope for the best without considering the "why's"

```{admonition} Discuss 
How is this different from how you've thougth about CS before?
```
- There's more to consider before actually coding
- Vast social influences in things as simple as "can i even get this dataset"
- We focus on what works now instead of what hasn't worked

```{admonition} Discuss 
How might the value-laden points about theory development relate the the scientific method points
```
- Two pillars of hypothesis + statistical testing

## Meta points
- (science) scientific mehtod one is a workshop paper (approx length of your project papers)
- this is a position paper (it's not about new experimental results as much as the classic research arguing a position apper)

## closing 

- volunteer: Emmely
- you can use this notes if you like 
- I can provide you "TA" access to prismia to draw there

  - [Roles for computing in social change](https://dl.acm.org/doi/abs/10.1145/3351095.3372871
  - See course site for notes on [expectations during presentations](https://ml4scisoc.github.io/syllabus/learning.html#presentations)

-----------------------------------------

# 2022-01-26

Lead Scribe: Lily

## Admin

- sorry about notes
- private github repo --> Spring 2022
- grading contract FYI 
    - Will be given further instructions on ways to achieve an 'A' or 'B'
        - To get a 'B' you will only need to complete the paper and presentation
        - To get an 'A' you will implement a project (translation)
    - Paper and presentation will be assigned
        - Paper --> CS Conference Style
        - Draft due: last day before the presentation, will be posted for the class to review


## Opening Question 

What kind of data are you most in working with?

- Class response:
    - GIS data
    - Linguistic data (tweets, reddit posts)
    - Numerical data (tabular)
    - Video/Image
    - Time series 
    - EHR/Medical related data
    - NLP
    - tabular/survey
    

## How to Read a Paper


## Model Based ML
- Discrete probabilities (distributions introduced in murder mystery chapter)
- Bernoulli
- Priors (probablistic guess about a random variable)
    - Are useful for working with less data to create strong inferences
        - Working with things when not a lot of data is available
    - Assumptions, expressed in a probability distribution
- Posterior
    - Inference given regularizer: Likelihood...
    - Most common posterior probability distribution we're doing: Probability of parameters given data
- Point Estimate
    - This are the single values produced after training (weights)
    - Posterior mean
- Most of the probability distributions we'll use belong to the exponential family 
    - https://en.wikipedia.org/wiki/Exponential_family
- Conditional Probability
    - One for each value of the conditioning variable
    - (e.g.) Murder mystery --> murderer variable can be Grey or Auburn
- Marginal probability
    - (Section 1.2 -- A Model of Murder)
    - "Probability of one event in the presence of all (or subset) outcomes of the other random variable..." (https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/)
- Maximum Likelihood Estimation
    - Assume a distribution, our goal will be to find the theta (parameter)
    - Maximizing, find parameters that will give us the highest probability (finding the one--parameter--that fits best)
- elicitation - study of how to elicit...


## Prepare for next class
- Order of the weekly topics may change
- Dr. Brown will present next week, but we'll start rotating the following week
- There are (2) readings, bring questions and prepare

### Learning & Evaluation

- Read through the whole Learning and Evaluation Page after I post a notification to, there are some fixes to be made
- Bring Questions to class next week
- Be ready to work on your grading contract

### Reading

The Scientific Method in the Science of Machine Learning and Value-laden Disciplinary Shifts in Machine Learning

### 


